{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP2+w0PyfYOGyujIs5jZ/Uo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bZu80Is4brWp","executionInfo":{"status":"ok","timestamp":1698291658629,"user_tz":240,"elapsed":166,"user":{"displayName":"Brielle Hina","userId":"05130376381506786767"}},"outputId":"175c1983-56a2-4822-a077-882e8ec74b66"},"outputs":[{"output_type":"stream","name":"stdout","text":["Selected feature to split: x1\n","Information Gain: 3.321928094887362\n","Entropy at this node: 1.0\n","Selected feature to split: x2\n","Information Gain: 2.584962500721156\n","Entropy at this node: 0.9182958340544896\n","Selected feature to split: x3\n","Information Gain: 2.584962500721156\n","Entropy at this node: 0.9182958340544896\n","Selected feature to split: x4\n","Information Gain: 2.584962500721156\n","Entropy at this node: 0.9182958340544896\n","Selected feature to split: x2\n","Information Gain: 2.0\n","Entropy at this node: 0.8112781244591328\n","Selected feature to split: x3\n","Information Gain: 2.0\n","Entropy at this node: 0.8112781244591328\n","Selected feature to split: x4\n","Information Gain: 2.0\n","Entropy at this node: 0.8112781244591328\n","Final Decision Tree:\n","{'x1': {0: {'x2': {0: {'x3': {0: {'x4': {0: -1}}}}}}, 1: {'x2': {1: {'x3': {1: {'x4': {1: 1}}}}}}}}\n"]}],"source":["import numpy as np\n","import math\n","\n","def entropy(y):\n","    unique_vals, counts = np.unique(y, return_counts=True)\n","    total_instances = len(y)\n","    entropy = 0\n","    for count in counts:\n","        p = count / total_instances\n","        entropy -= p * math.log2(p)\n","    return entropy\n","\n","def information_gain(S, y):\n","    total_entropy = entropy(y)\n","    unique_values = np.unique(S)\n","    weighted_entropy = 0\n","    for value in unique_values:\n","        subset_y = y[S == value]\n","        weighted_entropy += (len(subset_y) / len(y)) * entropy(subset_y)\n","    return total_entropy - weighted_entropy\n","\n","def id3(S, y, features):\n","    # if all the instances left have the same target, then give a leave node to that target value\n","    if len(np.unique(y)) == 1:\n","        return y[0]\n","\n","    #if there is nothing left to split on then return the class with the most data\n","    if len(features) == 0:\n","        unique_labels, counts = np.unique(y, return_counts=True)\n","        return unique_labels[np.argmax(counts)]\n","\n","    #calculate best info gain\n","    best_feature = None\n","    best_info_gain = -1\n","    for feature in features:\n","        info_gain = information_gain(feature, y)\n","        if info_gain > best_info_gain:\n","            best_info_gain = info_gain\n","            best_feature = feature\n","\n","    print(f\"Selected feature to split: {best_feature}\")\n","    print(f\"Information Gain: {best_info_gain}\")\n","    print(f\"Entropy at this node: {entropy(y)}\")\n","\n","    tree = {best_feature: {}}\n","    remaining_features = [f for f in features if f != best_feature]\n","    for value in np.unique(S):\n","        subset_indices = np.where(S == value)[0]\n","        subset_y = y[subset_indices]\n","        if len(subset_y) == 0:\n","            unique_labels, counts = np.unique(y, return_counts=True)\n","            tree[best_feature][value] = unique_labels[np.argmax(counts)]\n","        else:\n","            tree[best_feature][value] = id3(S[subset_indices], subset_y, remaining_features)\n","\n","    return tree\n","\n","\n","x1 = np.array([0, 1, 1, 0, 1, 0, 0, 0, 1, 0])\n","x2 = np.array([1, 0, 1, 0, 1, 0, 0, 0, 0, 0])\n","x3 = np.array([0, 1, 1, 0, 1, 1, 0, 1, 0, 1])\n","x4 = np.array([1, 0, 0, 1, 0, 1, 0, 0, 0, 1])\n","y = np.array([1, 1, 1, 1, 1, -1, -1, -1, -1, -1])\n","features = ['x1', 'x2', 'x3', 'x4']\n","decision_tree = id3(x1, y, features)\n","print(\"Final Decision Tree:\")\n","print(decision_tree)"]}]}